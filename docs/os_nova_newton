
Add perf event support for libvirt driver. This can be done by adding new configure option ‘enabled_perf_events’ in libvirt section of nova.conf. This feature requires libvirt>=2.0.0.

Starting from REST API microversion 2.34 pre-live-migration checks are performed asynchronously. instance-actions should be used for getting information about the checks results. New approach allows to reduce rpc timeouts amount, as previous workflow was fully blocking and checks before live-migration make blocking rpc request to both source and destination compute node.

New configuration option live_migration_permit_auto_converge has been added to allow hypervisor to throttle down CPU of an instance during live migration in case of a slow progress due to high ratio of dirty pages. Requires libvirt>=1.2.3 and QEMU>=1.6.0.

New configuration option live_migration_permit_post_copy has been added to start live migrations in a way that allows nova to switch an on-going live migration to post-copy mode. Requires libvirt>=1.3.3 and QEMU>=2.5.0. If post copy is permitted and version requirements are met it also changes behaviour of ‘live_migration_force_complete’, so that it switches on-going live migration to post-copy mode instead of pausing an instance during live migration.

Fix os-console-auth-tokens API to return connection info for all types of tokens, not just RDP.

Hyper-V RemoteFX feature.
Microsoft RemoteFX enhances the visual experience in RDP connections,
including providing access to virtualized instances of a physical GPU to
multiple guests running on Hyper-V.
In order to use RemoteFX in Hyper-V 2012 R2, one or more DirectX 11
capable display adapters must be present and the RDS-Virtualization
server feature must be installed.
To enable this feature, the following config option must be set in
the Hyper-V compute node’s ‘nova.conf’ file:
[hyperv]
enable_remotefx = True


To create instances with RemoteFX capabilities, the following flavor
extra specs must be used:
os:resolution. Guest VM screen resolution size. Acceptable values:
1024x768, 1280x1024, 1600x1200, 1920x1200, 2560x1600, 3840x2160


‘3840x2160’ is only available on Windows / Hyper-V Server 2016.
os:monitors. Guest VM number of monitors. Acceptable values:
[1, 4] - Windows / Hyper-V Server 2012 R2
[1, 8] - Windows / Hyper-V Server 2016


os:vram. Guest VM VRAM amount. Only available on
Windows / Hyper-V Server 2016. Acceptable values:
64, 128, 256, 512, 1024


There are a few considerations that needs to be kept in mind:

Not all guests support RemoteFX capabilities.

Windows / Hyper-V Server 2012 R2 does not support Generation 2 VMs
with RemoteFX capabilities.

Per resolution, there is a maximum amount of monitors that can be
added. The limits are as follows:
For Windows / Hyper-V Server 2012 R2:
1024x768: 4
1280x1024: 4
1600x1200: 3
1920x1200: 2
2560x1600: 1


For Windows / Hyper-V Server 2016:
1024x768: 8
1280x1024: 8
1600x1200: 4
1920x1200: 4
2560x1600: 2
3840x2160: 1






Not all guests support RemoteFX capabilities.

Windows / Hyper-V Server 2012 R2 does not support Generation 2 VMs
with RemoteFX capabilities.

Per resolution, there is a maximum amount of monitors that can be
added. The limits are as follows:
For Windows / Hyper-V Server 2012 R2:
1024x768: 4
1280x1024: 4
1600x1200: 3
1920x1200: 2
2560x1600: 1


For Windows / Hyper-V Server 2016:
1024x768: 8
1280x1024: 8
1600x1200: 4
1920x1200: 4
2560x1600: 2
3840x2160: 1




Microversion v2.26 allows to create/update/delete simple string tags. They can be used for filtering servers by these tags.

Added microversion v2.35 that adds pagination support for keypairs with the help of new optional parameters ‘limit’ and ‘marker’ which were added to GET /os-keypairs request.

Added microversion v2.28 from which hypervisor’s ‘cpu_info’ field returned as JSON object by sending GET /v2.1/os-hypervisors/{hypervisor_id} request.

Virtuozzo Storage is available as a volume backend in
libvirt virtualization driver.

Note
Only qcow2/raw volume format supported, but not ploop.



Virtuozzo ploop disks can be resized now during “nova resize”.

Virtuozzo instances with ploop disks now support the rescue operation

A new nova-manage command has been added to discover any new hosts that are added to a cell. If a deployment has migrated to cellsv2 using either the simple_cell_setup or the map_cell0/map_cell_and_hosts/map_instances combo then anytime a new host is added to a cell this new “nova-manage cell_v2 discover_hosts” needs to be run before instances can be booted on that host. If multiple hosts are added at one time the command only needs to be run one time to discover all of them. This command should be run from an API host, or a host that is configured to use the nova_api database.
Please note that adding a host to a cell and not running this command could lead to build failures/reschedules if that host is selected by the scheduler. The discover_hosts command is necessary to route requests to the host but is not necessary in order for the scheduler to be aware of the host. It is advised that nova-compute hosts are configured with “enable_new_services=False” in order to avoid failures before the hosts have been discovered.

On evacuate actions, the default behaviour when providing a host in the request body changed. Now, instead of bypassing the scheduler when asking for a destination, it will instead call it with the requested destination to make sure the proposed host is accepted by all the filters and the original request. In case the administrator doesn’t want to call the scheduler when providing a destination, a new request body field called force (defaulted to False) will modify that new behaviour by forcing the evacuate operation to the destination without verifying the scheduler.

On live-migrate actions, the default behaviour when providing a host in the request body changed. Now, instead of bypassing the scheduler when asking for a destination, it will instead call it with the requested destination to make sure the proposed host is accepted by all the filters and the original request. In case the administrator doesn’t want to call the scheduler when providing a destination, a new request body field called force (defaulted to False) will modify that new behaviour by forcing the live-migrate operation to the destination without verifying the scheduler.

The 2.37 microversion adds support for automatic allocation of network
resources for a project when networks: auto is specified in a server
create request. If the project does not have any networks available to it
and the auto-allocated-topology API is available in the Neutron
networking service, Nova will call that API to allocate resources for the
project. There is some setup required in the deployment for the
auto-allocated-topology API to work in Neutron. See the
Additional features section of the OpenStack Networking Guide
for more details for setting up this feature in Neutron.

Note
The API does not default to ‘auto’. However, python-novaclient
will default to passing ‘auto’ for this microversion if no specific
network values are provided to the CLI.


Note
This feature is not available until all of the compute services
in the deployment are running Newton code. This is to avoid sending a
server create request to a Mitaka compute that can not understand a
network ID of ‘auto’ or ‘none’. If this is the case, the API will treat
the request as if networks was not in the server create request body.
Once all computes are upgraded to Newton, a restart of the nova-api
service will be required to use this new feature.



Nova now defaults to using the glance version 2 protocol for all backend operations for all virt drivers. A use_glance_v1 config option exists to revert to glance version 1 protocol if issues are seen, however that will be removed early in Ocata, and only glance version 2 protocol will be used going forward.

Adds a new feature to the ironic virt driver, which allows
multiple nova-compute services to be run simultaneously. This uses
consistent hashing to divide the ironic nodes between the nova-compute
services, with the hash ring being refreshed each time the resource tracker
runs.
Note that instances will still be owned by the same nova-compute service
for the entire life of the instance, and so the ironic node that instance
is on will also be managed by the same nova-compute service until the node
is deleted. This also means that removing a nova-compute service will
leave instances managed by that service orphaned, and as such most
instance actions will not work until a nova-compute service with the same
hostname is brought (back) online.
When nova-compute services are brought up or down, the ring will eventually
re-balance (when the resource tracker runs on each compute). This may
result in duplicate compute_node entries for ironic nodes while the
nova-compute service pool is re-balancing. However, because any
nova-compute service running the ironic virt driver can manage any ironic
node, if a build request goes to the compute service not currently managing
the node the build request is for, it will still succeed.
There is no configuration to do to enable this feature; it is always
enabled.  There are no major changes when only one compute service is
running. If more compute services are brought online, the bigger changes
come into play.
Note that this is tested when running with only one nova-compute service,
but not more than one. As such, this should be used with caution for
multiple compute hosts until it is properly tested in CI.


Multitenant networking for the ironic compute driver is now supported. To enable this feature, ironic nodes must be using the ‘neutron’ network_interface.

The Libvirt driver now uses os-vif plugins for handling plug/unplug actions for the Linux Bridge and OpenVSwitch VIF types. Each os-vif plugin will have its own group in nova.conf for configuration parameters it needs. These plugins will be installed by default as part of the os-vif module installation so no special action is required.

Added hugepage support for POWER architectures.

Microversions may now (with microversion 2.27) be requested with the “OpenStack-API-Version: compute 2.27” header, in alignment with OpenStack-wide standards. The original format, “X-OpenStack-Nova-API-Version: 2.27”, may still be used.

Nova has been enabled for mutable config. Certain options may be reloaded
by sending SIGHUP to the correct process. Live migration options will apply
to live migrations currently in progress. Please refer to the configuration
manual.
DEFAULT.debug
libvirt.live_migration_completion_timeout
libvirt.live_migration_progress_timeout



DEFAULT.debug
libvirt.live_migration_completion_timeout
libvirt.live_migration_progress_timeout

The following legacy notifications have been been transformed to
a new versioned payload:

instance.delete
instance.pause
instance.power_on
instance.shelve
instance.suspend
instance.restore
instance.resize
instance.update
compute.exception

Every versioned notification has a sample file stored under
doc/notification_samples directory. Consult
http://docs.openstack.org/developer/nova/notifications.html for more information.


instance.delete
instance.pause
instance.power_on
instance.shelve
instance.suspend
instance.restore
instance.resize
instance.update
compute.exception

Nova is now configured to work with two oslo.policy CLI scripts that have been added.
The first of these can be called like “oslopolicy-list-redundant –namespace nova” and will output a list of policy rules in policy.[json|yaml] that match the project defaults. These rules can be removed from the policy file as they have no effect there.
The second script can be called like “oslopolicy-policy-generator –namespace nova –output-file policy-merged.yaml” and will populate the policy-merged.yaml file with the effective policy. This is the merged results of project defaults and config file overrides.

Added microversion v2.33 which adds paging support for hypervisors, the admin is able to perform paginate query by using limit and marker to get a list of hypervisors. The result will be sorted by hypervisor id.

The nova-compute worker now communicates with the new placement API service. Nova determines the placement API service by querying the OpenStack service catalog for the service with a service type of ‘placement’. If there is no placement entry in the service catalog, nova-compute will log a warning and no longer try to reconnect to the placement API until the nova-worker process is restarted.

A new [placement] section is added to the nova.conf configuration file for configuration options affecting how Nova interacts with the new placement API service. This contains the usual keystone auth and session options.

The pointer_model configuration option and hw_pointer_model image property was added to specify different pointer models for input devices. This replaces the now deprecated use_usb_tablet option.

The nova-policy command line is implemented as a tool to experience the under-development feature policy discovery. User can input the credentials infomation and the instance info, the tool will return a list of API which can be allowed to invoke. There isn’t any contract for the interface of the tool due to the feature still under-development.

Add a nova-manage command to refresh the quota usages for a project or user.  This can be used when the usages in the quota-usages database table are out-of-sync with the actual usages.  For example, if a resource usage is at the limit in the quota_usages table, but the actual usage is less, then nova will not allow VMs to be created for that project or user. The nova-manage command can be used to re-sync the quota_usages table with the actual usage.

Libvirt driver will attempt to update the time of a suspended and/or a migrated guest in order to keep the guest clock in sync. This operation will require the guest agent to be configured and running in order to be able to run. However, this operation will not be disruptive.

This release includes a new implementation of the vendordata metadata system. Please see the blueprint at http://specs.openstack.org/openstack/nova-specs/specs/newton/approved/vendordata-reboot.html for a detailed description. There is also documentation in the Nova source tree in vendordata.rst.

The 2.32 microversion adds support for virtual device
role tagging. Device role tagging is an answer to the
question ‘Which device is which, inside the guest?’ When
booting an instance, an optional arbitrary ‘tag’
parameter can be set on virtual network interfaces
and/or block device mappings. This tag is exposed to the
instance through the metadata API and on the config
drive. Each tagged virtual network interface is listed
along with information about the virtual hardware, such
as bus type (ex: PCI), bus address (ex: 0000:00:02.0),
and MAC address. For tagged block devices, the exposed
hardware metadata includes the bus (ex: SCSI), bus
address (ex: 1:0:2:0) and serial number.
The 2.32 microversion also adds the 2016-06-30 version
to the metadata API. Starting with 2016-06-30, the
metadata contains a ‘devices’ sections which lists any
devices that are tagged as described in the previous
paragraph, along with their hardware metadata.

